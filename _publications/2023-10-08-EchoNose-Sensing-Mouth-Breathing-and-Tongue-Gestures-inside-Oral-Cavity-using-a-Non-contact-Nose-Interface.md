---
title: "EchoNose: Sensing Mouth, Breathing and Tongue Gestures inside Oral Cavity using a Non-contact Nose Interface"
collection: publications
permalink: /publication/2023-10-08-EchoNose-Sensing-Mouth-Breathing-and-Tongue-Gestures-inside-Oral-Cavity-using-a-Non-contact-Nose-Interface
excerpt: 'October 8-12, 2023, Cancún, Mexico. Keyword: Nose Interface, Tongue Gestures, Breathing Patterns, Silent Speech, Acoustic Sensing'
date: 2023-10-08
venue: 'The Proceedings of the International Symposium on Wearable Computers (ISWC)'
paperurl: 'https://dl.acm.org/doi/10.1145/3594738.3611358'
citation: 'Rujia Sun, Xiaohe Zhou, Benjamin Steeper, Ruidong Zhang, Sicheng Yin, <u>Ke Li</u>, Shengzhang Wu, Sam Tilsen, François Guimbretière, and Cheng Zhang. 2023. EchoNose: Sensing Mouth, Breathing and Tongue Gestures inside Oral Cavity using a Non-contact Nose Interface. In <i>Proceedings of the International Symposium on Wearable Computers (ISWC), October 8-12, 2023, Cancún, Mexico</i>. ACM, New York, NY, USA, 22-26.'
---
Selected Media Coverage:

October 8-12, 2023, Cancún, Mexico
Keyword: Nose Interface, Tongue Gestures, Breathing Patterns, Silent Speech, Acoustic Sensing

<figure>
    <center><img src="https://keli97.github.io/files/echonose.png" alt="Trulli" style="width:100%" class="center"></center>
</figure>

Sensing movements and gestures inside the oral cavity has been a long-standing challenge for the wearable research community. This paper introduces EchoNose, a novel nose interface that explores a unique sensing approach to recognize gestures related to mouth, breathing, and tongue by analyzing the acoustic signal reflections inside the nasal and oral cavities. The interface incorporates a speaker and a microphone placed at the nostrils, emitting inaudible acoustic signals and capturing the corresponding reflections. These received signals were processed using a customized data processing and machine learning pipeline, enabling the distinction of 16 gestures involving speech, tongue, and breathing. A user study with 10 participants demonstrates that EchoNose achieves an average accuracy of 93.7% in recognizing these 16 gestures. Based on these promising results, we discuss the potential opportunities and challenges associated with applying this innovative nose interface in various future applications.

<!--<iframe width="420" height="315"-->
<!--src="https://www.youtube.com/embed/ZjucAwFqVqQ">-->
<!--</iframe>-->
