---
title: "ActSonic: Recognizing Everyday Activities from Inaudible Acoustic Waves Around the Body"
collection: publications
permalink: /publication/2024-09-25-ActSonic-Recognizing-Everyday-Activities-from-Inaudible-Acoustic-Waves-Around-the-Body
excerpt: 'October 2025 (Expected). Keyword: Activity Recognition, Smart Glasses, Acoustic Sensing, User-independent'
date: 2024-09-25
venue: 'The Proceedings of the Association for Computing Machinery on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)/UbiComp'
paperurl: 'https://arxiv.org/abs/2404.13924'
citation: 'Saif Mahmud, Vineet Parikh, Qikang Liang, <u>Ke Li</u>, Ruidong Zhang, Ashwin Ajit, Vipin Gunda, Devansh Agarwal, François Guimbretière, and Cheng Zhang. 2024. ActSonic: Recognizing Everyday Activities from Inaudible Acoustic Waves Around the Body. <i>The Proceedings of the Association for Computing Machinery on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)/UbiComp</i>. (Accepted Pending Minor Revisions)'
---
<!--Selected Media Coverage:-->

October 2025 (Expected)<br>
Keyword: Activity Recognition, Smart Glasses, Acoustic Sensing, User-independent

<figure>
    <center><img src="https://keli97.github.io/files/actsonic.png" alt="Trulli" style="width:100%" class="center"></center>
</figure>

We present ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses that can recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing) from inaudible acoustic waves around the body with a time resolution of one second. It only needs a pair of miniature speakers and microphones mounted on each hinge of eyeglasses to emit ultrasonic waves to create an acoustic aura around the body. Based on the position and motion of various body parts, the acoustic signals are reflected with unique patterns captured by the microphone and analyzed by a customized self-supervised deep learning framework to infer the performed activities. ActSonic was deployed in a user study with 19 participants across 19 households to evaluate its efficacy. Without requiring any training data from a new user (leave-one-participant-out evaluation), ActSonic was able to detect 27 activities, achieving an average F1-score of 86.6% in fully unconstrained scenarios and 93.4% in prompted settings at participants’ homes.

<!--<iframe width="420" height="315"-->
<!--src="https://www.youtube.com/embed/XvNLNkfQY7Q">-->
<!--</iframe>-->
